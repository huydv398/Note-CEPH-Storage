# Cài đặt CEPH ALL IN ONE In CentOS-7
## Chuẩn bị cấu hình
- Server chạy CentOS-7 64 bit
- Tài nguyên:
    - CPU: 2 Core
    - RAM: 4 GB
    - Disk
        - OS: 10GB
        - DISK OSD: 3 x 50GB
- Network: 
    - Eth0: Access Ceph - 172.16.70.234/24
    - Eth1: Relicate data - 172.16.71.234/24

### Cấu hình cho Card mạng

1. Cấu hình Eth0
```
echo "Setup IP eth0"
nmcli conn modify eth0 ipv4.addresses 172.16.70.234/24 ipv4.gateway 172.16.70.1 ipv4.dns 8.8.8.8 ipv4.method manual connection.autoconnect yes
``` 
2. Cấu hình Eth1
```
echo "Setup IP eth1"
nmcli conn modify eth1 ipv4.addresses 172.16.71.234/24 ipv4.method manual connection.autoconnect yes
``` 
3. Update hệ thống và cài python-tools
```
yum install update -y
yum install python-setuptools -y

```
## Cài đặt
Sử dụng user `root` thực hiện cấu hình hoặc quyền tương đương.
### Phần 1 - Cấu hình ban đầu.

### 1.1 Tạo CEPH User
Tạo Ceph user 'cephuser' trên node.
```
# Tạo user 
useradd -d /home/cephuser -m cephuser
## đặt pass cho user 
passwd cephuser
```

Lưu ý:
* User được sử dụng bởi `Ceph-deloy`. Tức, `Ceph-deloy` sẽ sử dụng user để triển khai các cấu hình của CEPH.
* Nếu không tạo User này, `Ceph-deloy` vẫn chạy, Nó sẽ tự động sử dụng biến user môi trường

Cấp quyền root cho user vừa tạo:
```
echo "cephuser ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephuser
chmod 0440 /etc/sudoers.d/cephuser
sed -i s'/Defaults requiretty/#Defaults requiretty'/g /etc/sudoers

```

### 1.2 Cấu hình NTP
Sử dụng NTP đồng bộ thời gian trên tất cả các Node.

Ở đây sử dụng NTP pool US.
```
yum install -y ntp ntpdate ntp-doc
ntpdate 0.us.pool.ntp.org
hwclock --systohc
systemctl enable ntpd.service
systemctl start ntpd.service
```
### 1.3 Hủy bỏ SELinux
```
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
```
### 1.4 Cấu hình Host File
```
echo '
192.168.2.100 cephaio' >> /etc/hosts
```

Ping thử tới host, kiếm tra Network

```
[root@cephaio cluster]# ping cephaio
PING cephaio (172.16.70.234) 56(84) bytes of data.
64 bytes from cephaio (172.16.70.234): icmp_seq=1 ttl=64 time=0.070 ms
64 bytes from cephaio (172.16.70.234): icmp_seq=2 ttl=64 time=0.064 ms
```

### 1.5 Khởi động lại node
```
init 6
```

## Phần 2 cấu hình SSH Server 
Thực hiện với user `root`
### 2.1 Tạo ssh-key
```
echo -e "\n" | ssh-keygen -t rsa -N ""
```
### 2.2 Cấu hình ssh file
```
echo '
Host cephaio
        Hostname cephaio
        User cephuser' > ~/.ssh/config
```

Thay đổi quyền trên file
```
chmod 644 ~/.ssh/config
```
Thiết lập keypair
```
ssh-keyscan cephaio >> ~/.ssh/known_hosts
ssh-copy-id cephaio
```
>Yều cầu nhập password lần đầu tiền truy cập

## Phần 3: Thiết lập Ceph Cluster
### 3.1 Cấu hình Ceph repo
```
echo '
[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-luminous/el7/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-luminous/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-luminous/el7/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1' > /etc/yum.repos.d/ceph.repo
```

Cập nhật lại hệ thống:
```
yum update -y 
```
### 3.2 Cài đặt ceph-deploy
```
yum install ceph-deploy -y
yum update ceph-deploy -y 
```
> Nếu không cài `python-setuptools` thực hiện sẽ bị lỗi

### 3.3 Tạo mới Ceph Cluster
Tạo cluster directory
```
mkdir cluster
cd cluster/
```

Tạo mới cấu hình ceph, thiết lập node mon
```
ceph-deploy new cephaio
```
Cấu hình ceph.conf
```
#2 dải mạng đang sử dụng cho CEPH
echo "public network = 172.16.70.0/24" >> ~/cluster/ceph.conf
echo "cluster network = 172.16.71.0/24" >> ~/cluster/ceph.conf
```
### 3.4 Cài đặt Ceph
```
ceph-deploy install --release luminous cephaio
```
Khởi tạo tiến trình mon
```
ceph-deploy mon create-initial
```

### 3.5 Thiết lập OSD
Khởi tạo tiến trình OSD. Liệt kê OSD:
```
ceph-deploy disk list cephaio
```

Check tên partition bằng lệnh 
```
lsblk
```
Xóa partition tables trên tất cả node với zap option
```
ceph-deploy disk zap cephaio /dev/vdb
ceph-deploy disk zap cephaio /dev/vdc
ceph-deploy disk zap cephaio /dev/vdd
```
Tạo mới OSD
```
ceph-deploy osd create cephaio --data /dev/vdb
ceph-deploy osd create cephaio --data /dev/vdc
ceph-deploy osd create cephaio --data /dev/vdd
```
Kiểm tra tại OSD node
```

[root@cephaio cluster]# lsblk
NAME                                                                                       MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0                                                                                         11:0    1 1024M  0 rom
vda                                                                                        252:0    0   10G  0 disk
├─vda1                                                                                     252:1    0  512M  0 part /boot
└─vda2                                                                                     252:2    0  9.5G  0 part
  ├─centos-root                                                                            253:0    0  8.5G  0 lvm  /
  └─centos-swap                                                                            253:1    0    1G  0 lvm  [SWAP]
vdb                                                                                        252:16   0   50G  0 disk
└─ceph--d001aedc--36ac--46db--a746--cd4a96827863-osd--block--eee9dc66--29c7--4bfa--89e2--fa35f3e8bb7c
                                                                                           253:2    0   50G  0 lvm
vdc                                                                                        252:32   0   50G  0 disk
└─ceph--ac2839e2--c5f0--450f--86bd--d28fd9013d5b-osd--block--73084a09--b536--4159--8fc1--05f2076c4285
                                                                                           253:3    0   50G  0 lvm
vdd                                                                                        252:48   0   50G  0 disk
└─ceph--e030384a--c959--4a5c--a66f--f4d5add03fb8-osd--block--15f6f762--e99b--45d0--a811--515418557bbc
                                                                                           253:4    0   50G  0 lvm
```

Khởi tạo quyền admin ceph cho node `cephaio` (Để node có quyền quản trị Ceph)
```
ceph-deploy admin cephaio
```
Thiết lập quyền truy cập file
```
sudo chmod 644 /etc/ceph/ceph.client.admin.keyring
```
Khởi tạo tiến trình ceph mgr (Không khởi tạo có thể gây lỗi ceph)
```
ceph-deploy mgr create cephaio:ceph-mgr-1
```
## Phần 4: Kiểm tra trạng thái Ceph
Kiểm tra cluster health
```
[root@cephaio cluster]# ceph health
HEALTH_OK
```
Kiểm tra cluster status
```
[root@cephaio cluster]# ceph -s
  cluster:
    id:     349e31e9-496b-4cba-b3c0-58aa6d2dd5d4
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum cephaio
    mgr: ceph-mgr-1(active)
    osd: 3 osds: 3 up, 3 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0B
    usage:   3.01GiB used, 147GiB / 150GiB avail
    pgs:


```

## Phần 5: Chỉnh sửa Crush map
Khi cài đặt Ceph All In One, cần phải bổ sung cấu hình sau để lab các tính năng khác

Truy cập Cluster config, lấy crush map hiện tại
```
cd cluster/
ceph osd getcrushmap -o map.bin
crushtool -d map.bin -o map.txt
```

Chỉnh sửa crush map
```
sed -i 's/step chooseleaf firstn 0 type host/step chooseleaf firstn 0 type osd/g' ~/cluster/map.txt
crushtool -c map.txt -o map-new.bin
ceph osd setcrushmap -i map-new.bin
```

## Phần 6: Khởi tạo Ceph Dashboard
Sử dụng cho mục đính giám sát Ceph

Liệt kê các module hiện có
```
[root@cephaio cluster]# ceph mgr dump
{
    "epoch": 16,
    "active_gid": 4117,
    "active_name": "ceph-mgr-1",
    "active_addr": "172.16.70.234:6806/10280",
    "available": true,
    "standbys": [],
    "modules": [
        "balancer",
        "restful",
        "status"
    ],
    "available_modules": [
        "balancer",
        "dashboard",
        "influx",
        "localpool",
        "prometheus",
        "restful",
        "selftest",
        "status",
        "telemetry",
        "zabbix"
    ],
    "services": {}
}

```

Kích hoạt module dashboard
```
ceph mgr module enable dashboard
```

Kiểm tra lại

```
[root@cephaio cluster]# ceph mgr dump
{
    "epoch": 20,
    "active_gid": 4124,
    "active_name": "ceph-mgr-1",
    "active_addr": "172.16.70.234:6806/10280",
    "available": true,
    "standbys": [],
    "modules": [
        "balancer",
        "dashboard",
        "restful",
        "status"
    ],
    "available_modules": [
        "balancer",
        "dashboard",
        "influx",
        "localpool",
        "prometheus",
        "restful",
        "selftest",
        "status",
        "telemetry",
        "zabbix"
    ],
    "services": {
        "dashboard": "http://cephaio:7000/"
    }
}

```

Truy cập giao diện
```
http://<ip>:7000/
```

Hoặc
```
http://172.16.70.234:7000/
```